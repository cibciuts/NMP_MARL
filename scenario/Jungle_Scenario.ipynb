{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is the implementation of Jungle scenario of Neural Message Passing Reinforcement Learning (NMP-RL).\n",
    "The paper is submitted to Nature Machine Intelligence and under reviewed.\n",
    "\n",
    "Written by Kha Vo, Chin-Teng Lin, University of Technology Sydney\n",
    "\n",
    "For more magical stuffs on RL, please visit\n",
    "https://voanhkha.github.io/2019/11/29/magic_rl_p1/\n",
    "\n",
    "Email: kha.vo@uts.edu.au\n",
    "www.kaggle.com/khahuras\n",
    "www.github.com/voanhkha\n",
    "\n",
    "Gentle Request: I am happy if you re-use the materials here or the article\n",
    "Please just kindly cite it by simply copying the source link, or my name.\n",
    "So much thanks!\n",
    "Kind regards,\n",
    "Kha Vo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd, os, cv2\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from matplotlib.animation import FuncAnimation\n",
    "plt.style.use('seaborn-pastel')\n",
    "import time\n",
    "from IPython import display\n",
    "%matplotlib notebook\n",
    "%matplotlib notebook\n",
    "\n",
    "from itertools import count\n",
    "from collections import namedtuple\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "import schnetpack\n",
    "from schnetpack.nn import Dense, shifted_softplus\n",
    "from torch_scatter import scatter_add, scatter_mean\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "    \n",
    "    def plot(self, path='frames/sample.png', title='title', show_plot=True, save_fig=False):\n",
    "        colors = {'red':[230, 32, 32], 'green':[40,255,40]}\n",
    "        M = np.zeros((self.width, self.height, 3), dtype=np.uint8)\n",
    "        global_maps = [self.get_view(None, 0, t) for t in self.agent_types] \n",
    "        for m, c in zip(global_maps, colors):\n",
    "            for i in range(self.width):\n",
    "                for j in range(self.height):\n",
    "                    if m[i,j] != 0: M[i,j,:] = colors[c]\n",
    "                    #else:  M[i,j,:] = [255,255,255]\n",
    "        if show_plot:\n",
    "            plt.imshow(M) \n",
    "            plt.axis('off')\n",
    "            plt.suptitle(title, fontsize=16)\n",
    "            #plt.tight_layout()\n",
    "            #plt.subplots_adjust(top=0.88)\n",
    "        if save_fig:\n",
    "            plt.savefig(path, bbox_inches='tight')\n",
    "            #plt.show()\n",
    "        return M\n",
    "\n",
    "    def pad_width(self, vector, pad_width, iaxis, kwargs):\n",
    "        pad_value = kwargs.get('padder', 10)\n",
    "        vector[:pad_width[0]] = pad_value\n",
    "        vector[-pad_width[1]:] = pad_value    \n",
    "    \n",
    "    def __init__(self, width=20, height=20, agent_types=['predator', 'prey'], \n",
    "                 n_agents_each_type=[4,4], terminal_frame=20):\n",
    "        self.width, self.height = width, height\n",
    "        self.agent_types = agent_types\n",
    "        self.n_agent_types = len(self.agent_types)\n",
    "        self.n_agents_each_type = n_agents_each_type\n",
    "        self.food_cells = []\n",
    "        self.agents = {}\n",
    "        self.map =  { t: [ [ [ ] for j in range(width) ]  for i in range(height) ] for t in self.agent_types}\n",
    "        self.init_agents(init_pos='random')\n",
    "        self.move_offset = [[0,1],[0,-1],[1,0],[-1,0], [0,0]]\n",
    "        self.done = False\n",
    "        self.terminal_frame = terminal_frame\n",
    "        self.count_frame = 0\n",
    "        self.global_reward = 0\n",
    "        \n",
    "    def sample_random_actions(self):\n",
    "        acts = [random.choices( range(len(self.move_offset)), k=N ) \n",
    "                for N in self.n_agents_each_type]\n",
    "        return acts\n",
    "\n",
    "    def init_agents(self, init_pos):\n",
    "        if init_pos == 'random':\n",
    "            init_pos = random.sample([[x,y] for x in range(self.width) for y in range(self.height)], \n",
    "                                     np.sum(self.n_agents_each_type))\n",
    "        cnt = 0\n",
    "        for j, t in enumerate(self.agent_types):\n",
    "            self.agents[t] = []\n",
    "            for i in range(self.n_agents_each_type[j]):  \n",
    "                _p = np.array(init_pos[cnt])\n",
    "                new_agent = Agent(agent_type=t, pos=_p, identity=i)\n",
    "                self.agents[t].append(new_agent)\n",
    "                self.map[t][_p[0]][_p[1]].append(i)\n",
    "                if t=='prey': self.food_cells.append([_p[0], _p[1]])\n",
    "                cnt += 1\n",
    "                \n",
    "                \n",
    "    def get_view(self, pos, vr, agent_type): # if vr==0, get the whole map without padding (use for render)\n",
    "        stride = vr*2+1\n",
    "        if self.map[agent_type] == [[[] for i in range(self.width)] for j in range(self.height)]: \n",
    "            empty = True\n",
    "            view = np.zeros((self.width, self.height))\n",
    "        else:\n",
    "            empty = False\n",
    "            view = np.array(self.map[agent_type])\n",
    "            \n",
    "        # pad the borders with -1, with thickness = viewrange\n",
    "        if vr>0: \n",
    "            view_pad = np.pad(view, vr, self.pad_width, padder=-1)\n",
    "            view = view_pad[ pos[0]:pos[0]+stride , pos[1]:pos[1]+stride ]\n",
    "        if empty: local_view = view\n",
    "        else: local_view = np.array([[len(subitem) if subitem!=-1 else -1 for subitem in item] for item in view])\n",
    "        return local_view\n",
    "    \n",
    "    \n",
    "    def get_neighbours(self, pos):\n",
    "        offsets_to_check = [[0,1], [0,-1], [1,0], [-1,0], [0,0]]\n",
    "        neighbours = []\n",
    "        for offset in offsets_to_check:\n",
    "            check_pos = pos + offset\n",
    "            if check_pos[0] < 0 or check_pos[0] >= self.width: continue\n",
    "            if check_pos[1] < 0 or check_pos[1] >= self.height: continue\n",
    "            #print(check_pos[0], check_pos[1])\n",
    "            occupied_agents = self.map['predator'][check_pos[0]][check_pos[1]]\n",
    "            neighbours.extend(occupied_agents)\n",
    "        return neighbours\n",
    "        \n",
    "\n",
    "    def get_connectivity_pairs(self, mat):\n",
    "        clusters = []\n",
    "        processed = []\n",
    "        for cur_idx, m in enumerate(mat):\n",
    "            if cur_idx in processed: continue\n",
    "            neis = np.array(m)\n",
    "            neis = np.where(neis==1)[0].tolist()\n",
    "            if cur_idx == 0: \n",
    "                processed.append(cur_idx)\n",
    "                processed.extend(neis)\n",
    "                clusters.append([cur_idx] + neis)\n",
    "            else: \n",
    "                flag = False\n",
    "                for n in neis:\n",
    "                    for c, clus in enumerate(clusters):\n",
    "                        if n in clus: \n",
    "                            flag = True\n",
    "                            clusters[c].extend([cur_idx]+neis)\n",
    "                            processed.append(cur_idx)\n",
    "                            processed.extend(neis)\n",
    "                            break\n",
    "\n",
    "                    if flag: break\n",
    "\n",
    "                if not flag: \n",
    "                    processed.append(cur_idx)\n",
    "                    processed.extend(neis)\n",
    "                    clusters.append([cur_idx] + neis)\n",
    "\n",
    "        clusters = [np.unique(clus) for clus in clusters]\n",
    "        pairs = []\n",
    "        for clus in clusters:\n",
    "            pairs.extend([ [i,j] for i in clus for j in clus if i!=j ])\n",
    "        return pairs\n",
    "      \n",
    "    def dist(self, p0, p1):\n",
    "        return np.sqrt((p0[0]-p1[0])**2 + (p0[1]-p1[1])**2)\n",
    "    \n",
    "    def get_distances(self, conns):\n",
    "        dists = []\n",
    "        for conn in conns:\n",
    "            dists.append(self.dist( self.agents['predator'][conn[0]].pos, self.agents['predator'][conn[1]].pos ))\n",
    "        return dists\n",
    "        \n",
    "    \n",
    "    def build_connectivity(self):\n",
    "        ## Sub-graph\n",
    "#         connect_matrix = np.zeros((self.n_agents_each_type[0], self.n_agents_each_type[0]))\n",
    "#         for j, t in enumerate(self.agent_types):\n",
    "#             if t!='predator': continue\n",
    "#             for k, agent in enumerate(self.agents[t]):\n",
    "#                 neighbours = self.get_neighbours(agent.pos)\n",
    "#                 neighbours = [n for n in neighbours if n!=k]\n",
    "#                 for n in neighbours: connect_matrix[k, n] = 1     \n",
    "        \n",
    "#         connectivities = get_connectivity_pairs(connect_matrix)\n",
    "        \n",
    "        ## Full graph\n",
    "        nb_agents = self.n_agents_each_type[0]\n",
    "        \n",
    "        connectivities = [[i,j] for i in range(nb_agents) for j in range(nb_agents) if i!=j]\n",
    "        \n",
    "        distances = self.get_distances(connectivities)\n",
    "        \n",
    "#         print(connectivities, distances)\n",
    "        \n",
    "        return connectivities, distances\n",
    "        \n",
    "        \n",
    "    def transition(self, actions_input='random'):\n",
    "        if actions_input == 'random': actions_input = self.sample_random_actions()\n",
    "        self.count_frame += 1\n",
    "        # Move each agent\n",
    "        for j, t in enumerate(self.agent_types):\n",
    "            if t!='predator': continue\n",
    "                \n",
    "            for k, agent in enumerate(self.agents[t]):\n",
    "                \n",
    "                if agent.properties['movable'] is False: continue\n",
    "                if agent.properties['active'] is False: continue\n",
    "                current_pos = agent.pos\n",
    "                next_pos = current_pos + self.move_offset[actions_input[j][k]]\n",
    "\n",
    "                if [next_pos[0], next_pos[1]] in self.food_cells: # if agent overlap with food: don't move\n",
    "                    next_pos[0], next_pos[1] = current_pos[0], current_pos[1]\n",
    "                \n",
    "                elif next_pos[0] >= self.width or next_pos[0] < 0:\n",
    "                    next_pos[0] = current_pos[0]\n",
    "\n",
    "                elif next_pos[1] >= self.height or next_pos[1] < 0:\n",
    "                    next_pos[1] = current_pos[1]\n",
    "\n",
    "                agent.pos = next_pos\n",
    "                      \n",
    "                self.map[t][current_pos[0]][current_pos[1]].remove(agent.id)\n",
    "                self.map[t][next_pos[0]][next_pos[1]].append(agent.id)\n",
    "                \n",
    "                agent.current_reward = agent.properties['default_reward'] # reset reward before interact with environment\n",
    "\n",
    "\n",
    "        # Update predators\n",
    "        for j, t in enumerate(self.agent_types):\n",
    "            if t!='predator': continue\n",
    "                \n",
    "            for agent in self.agents[t]:\n",
    "                if agent.properties['active'] is False: continue # if agent is dead, skip\n",
    "                    \n",
    "                # Check if this predator is next to a food or not\n",
    "                view = self.get_view(agent.pos, agent.properties['viewrange'], 'prey')\n",
    "                next_to_food = True if view[0,1]>=1 or view[1,0]>=1 or view[1,2]>=1 or view[2,1]>=1 else False\n",
    "                if next_to_food: \n",
    "                    agent.properties['kills'] += 1\n",
    "                    agent.current_reward += 1\n",
    "                    \n",
    "                # Check if this predator is next to other 2 agents\n",
    "                view = self.get_view(agent.pos, agent.properties['viewrange'], 'predator')\n",
    "                next_1 = view[0,1] if view[0,1]!=-1 else 0\n",
    "                next_2 = view[1,0] if view[1,0]!=-1 else 0\n",
    "                next_3 = view[1,2] if view[1,2]!=-1 else 0\n",
    "                next_4 = view[2,1] if view[2,1]!=-1 else 0\n",
    "                next_5 = view[1,1]\n",
    "                surround_count = next_1+next_2+next_3+next_4+next_5-1 #-1 because view[1,1] is itself\n",
    "                #print(surround_count)\n",
    "                next_to_other = True if surround_count>=1 else False\n",
    "                if next_to_other: agent.properties['hp'] -= 1\n",
    "\n",
    "     \n",
    "        # Remove agents if hp exhausted by attacked\n",
    "        for j, t in enumerate(self.agent_types):\n",
    "            for agent in self.agents[t]:   \n",
    "                if agent.properties['active'] is False: continue\n",
    "                if agent.properties['hp'] <= 0:\n",
    "                    agent.properties['active'] = False\n",
    "                    agent.current_reward = -1\n",
    "                    self.map[t][agent.pos[0]][agent.pos[1]].remove(agent.id) # Remove agent from map\n",
    "                    \n",
    "            \n",
    "        # If exceed maximum frame, make the env \"done\"\n",
    "        if self.count_frame == self.terminal_frame+1: \n",
    "            self.done = True\n",
    "            #self.global_reward = -1 # count global reward??\n",
    "            #for agent in self.agents['predator']: agent.current_reward = -1\n",
    "                \n",
    "                \n",
    "class Agent:\n",
    "    def __init__(self, agent_type='predator', reward_rules={}, pos=[0,0], identity=-1,\n",
    "                properties={'movable':True, 'active':True}):\n",
    "        self.agent_type = agent_type\n",
    "        self.id = identity\n",
    "        if agent_type=='predator': properties ={'movable':True, 'active':True, \n",
    "                                                'viewrange':1, 'kills':0, \n",
    "                                                'hp':3, 'default_reward':0}\n",
    "        elif agent_type=='prey': properties ={'movable':False, 'active':True, \n",
    "                                              'viewrange':1, 'hp':1, 'default_reward':0}\n",
    "        self.properties = properties\n",
    "        self.pos = pos\n",
    "        self.action_buffer = []\n",
    "        self.view = None\n",
    "        self.current_reward = 0\n",
    "        self.reward_buffer = []\n",
    "        self.replay_buffer = []\n",
    "    \n",
    "    \n",
    "class Policy_Gradient(nn.Module):\n",
    "    def __init__(self, input_dim=25, output_dim=5):\n",
    "        super(Policy_Gradient, self).__init__()\n",
    "        self.affine = nn.Linear(input_dim, 128)\n",
    "        self.action_head = nn.Linear(128, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.affine(x))\n",
    "        action_prob = F.softmax(self.action_head(x), dim=-1)\n",
    "        return action_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN GRADIENT POLICY (REINFORCE)\n",
    "\n",
    "WIDTH, HEIGHT = 5, 5\n",
    "AGENT_TYPES = ['predator', 'prey']\n",
    "N_AGENTS_EACHTYPE = [4, 1]\n",
    "FRAMES_PER_EPISODE = 30\n",
    "EPISODES_TO_TRAIN = 12000\n",
    "VERBOSE = 100\n",
    "\n",
    "model = Policy_Gradient(input_dim = 18)\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "eps = np.finfo(np.float32).eps.item()\n",
    "\n",
    "# Main\n",
    "\n",
    "reward_log  = []\n",
    "\n",
    "for i_episode in range(EPISODES_TO_TRAIN):\n",
    "    # reset environment and episode reward\n",
    "    W = GridWorld(width=WIDTH, height=HEIGHT, agent_types=AGENT_TYPES, \n",
    "                  n_agents_each_type=N_AGENTS_EACHTYPE,\n",
    "                  terminal_frame=FRAMES_PER_EPISODE)\n",
    "    \n",
    "    ep_reward = 0\n",
    "    reward_buffer = [[] for _ in range(W.n_agents_each_type[0])] # each predator has 1 separate buffer\n",
    "    action_buffer = [[] for _ in range(W.n_agents_each_type[0])] # throughout the whole episode\n",
    "\n",
    "    while True: # transition the environment until done\n",
    "        actions = [[0 for i in range(N)] for N in W.n_agents_each_type] # init actions in right shape\n",
    "        \n",
    "        # Get action for each predator using the model\n",
    "        for agent_id, agent in enumerate(W.agents['predator']):\n",
    "            # TODO: only forward action of active predator\n",
    "            \n",
    "            view1 = W.get_view(agent.pos, agent.properties['viewrange'], 'prey').ravel()\n",
    "            view2 = W.get_view(agent.pos, agent.properties['viewrange'], 'predator').ravel()\n",
    "            state = torch.tensor(np.hstack([view1, view2])).float().to(device)\n",
    "            #print(state.shape)\n",
    "\n",
    "            probs = model(state) # forward pass\n",
    "            m = Categorical(probs)\n",
    "            action_sample = m.sample()\n",
    "            action_buffer[agent_id].append(m.log_prob(action_sample))\n",
    "            action = action_sample.item() # this \"action\" can be 0,1,2,3,4 (label encoded)\n",
    "\n",
    "            # embed action to the current agent\n",
    "            actions[0][agent_id] = action\n",
    "        \n",
    "        # transition\n",
    "        W.transition(actions)\n",
    "        \n",
    "        # Get reward for each predator from the new transitioned environment\n",
    "        for agent_id in range(W.n_agents_each_type[0]):\n",
    "            reward = W.agents['predator'][agent_id].current_reward\n",
    "            reward_buffer[agent_id].append(reward)\n",
    "        \n",
    "        if W.done: break\n",
    "\n",
    "            \n",
    "    # perform backprop\n",
    "    returns = []\n",
    "    for agent_id in list(range(W.n_agents_each_type[0]))[::-1]: # Reverse order\n",
    "        R = 0\n",
    "        for r in reward_buffer[agent_id][::-1]: # Reverse order\n",
    "            R = r + 0.99 * R\n",
    "            returns.insert(0, R)\n",
    "            \n",
    "    ep_reward = np.mean(returns[::len(reward_buffer[0])]   )\n",
    "    \n",
    "    returns = torch.tensor(returns)\n",
    "    #returns = (returns - returns.mean()) / (returns.std() + eps) # this one is suspicious\n",
    "\n",
    "    action_buffer = [item for sublist in action_buffer for item in sublist] # ravel\n",
    "    policy_losses = []\n",
    "    for log_prob, R in zip(action_buffer, returns):\n",
    "        policy_losses.append(-log_prob * R)\n",
    "\n",
    "\n",
    "    # reset gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # sum up all the values of policy_losses and value_losses\n",
    "    loss = torch.stack(policy_losses).sum() #+ torch.stack(value_losses).sum() # ?????\n",
    "\n",
    "    # perform backprop\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # log results\n",
    "            \n",
    "    \n",
    "    reward_log.append(ep_reward)\n",
    "    if (i_episode+1) % VERBOSE == 0: \n",
    "        print('Episode {}\\tReward: {:.2f}'.format(i_episode+1, np.mean(reward_log[-VERBOSE:])))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save plot\n",
    "train_curve_pg = np.reshape(reward_log, (-1,100)).mean(axis=1)\n",
    "plt.plot(train_curve_pg)\n",
    "# plt.savefig('revision_0_materials/PG_Jungle.png')\n",
    "\n",
    "# # Save numerical result\n",
    "# np.save('revision_0_materials/PG_Jungle.npy', np.array(reward_log))\n",
    "\n",
    "# # Save model\n",
    "# save_path = \"revision_0_materials/PG_Jungle_model.pth\"\n",
    "# torch.save(model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_stacked_offsets(sizes, repeats):\n",
    "    return np.repeat(np.cumsum(np.hstack([0, sizes[:-1]])), repeats)\n",
    "\n",
    "def _concat(to_stack):\n",
    "    \"\"\" function to stack (or concatentate) depending on dimensions \"\"\"\n",
    "    if np.asarray(to_stack[0]).ndim >= 2:\n",
    "        return np.concatenate(to_stack)\n",
    "    else:\n",
    "        return np.hstack(to_stack)\n",
    "\n",
    "def rbf_expansion(distances, mu=0, delta=0.1, kmax=150):\n",
    "    k = np.arange(0, kmax)\n",
    "    logits = -(np.atleast_2d(distances).T - (-mu + delta * k)) ** 2 / delta\n",
    "    return np.exp(logits)\n",
    "\n",
    "class SchnetWithEdgeUpdate(nn.Module):\n",
    "    def __init__(self, n_atom_basis=128, max_z=100, kmax=150, n_interactions=1, activation=shifted_softplus):\n",
    "        super(SchnetWithEdgeUpdate, self).__init__()\n",
    "        \n",
    "        self.n_interactions = n_interactions\n",
    "        \n",
    "        self.edge_update_net = nn.Sequential(\n",
    "            Dense(164, 32, activation=activation), # 164 = 128 (rbf expand) + 18 (state src) + 18 (state dst)\n",
    "            Dense(32, 18))\n",
    "        \n",
    "        self.msg_edge_net = nn.Sequential(\n",
    "            Dense(18, 18, activation=activation),\n",
    "            Dense(18, 18, activation=activation))\n",
    "        \n",
    "        self.msg_atom_fc = Dense(18, 18)\n",
    "        \n",
    "        self.state_trans_net = nn.Sequential(\n",
    "            Dense(18, 18, activation=activation),\n",
    "            Dense(18, 18))\n",
    "        \n",
    "        self.init_edge_fc = Dense(kmax, n_atom_basis, activation=activation)\n",
    "\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x_atom = inputs['states']\n",
    "#         print('\\n\\n')\n",
    "#         print('x_atom', x_atom.shape)\n",
    "        x_bond =  rbf_expansion(inputs['distance']).astype(np.float32)\n",
    "#         print('x_bond1', x_bond.shape)\n",
    "        x_bond = torch.from_numpy(x_bond)\n",
    "        x_bond = self.init_edge_fc(x_bond)\n",
    "#         print('x_bond2', x_bond.shape)\n",
    "#         print('conns', inputs['connectivity'])\n",
    "\n",
    "        src_idx = torch.LongTensor(inputs['connectivity'][:, 0])\n",
    "        dst_idx = torch.LongTensor(inputs['connectivity'][:, 1])\n",
    "#         print('src_idx', src_idx, 'dst_idx', dst_idx)\n",
    "# \n",
    "        for n in range(self.n_interactions):\n",
    "            # Update edge\n",
    "            x_src_atom = x_atom[src_idx]\n",
    "            x_dst_atom = x_atom[dst_idx]\n",
    "            x_bond = torch.cat((x_src_atom, x_dst_atom, x_bond), dim=1)\n",
    "#             print('x_bond3', x_bond.shape)\n",
    "            x_bond = self.edge_update_net(x_bond)\n",
    "#             print('x_bond4', x_bond.shape)\n",
    "\n",
    "            # message function\n",
    "            bond_msg = self.msg_edge_net(x_bond)\n",
    "#             print('bond_msg', bond_msg.shape)\n",
    "            src_atom_msg = self.msg_atom_fc(x_src_atom)\n",
    "#             print('src_atom_msg', src_atom_msg.shape)\n",
    "            messages = torch.mul(bond_msg, src_atom_msg)\n",
    "#             print('messages1', messages.shape)\n",
    "            messages = scatter_add(messages, dst_idx, dim=0)\n",
    "#             print('messages2', messages.shape)\n",
    "\n",
    "            # state transition function\n",
    "            messages = self.state_trans_net(messages)\n",
    "#             print('messages3', messages.shape)\n",
    "\n",
    "#             print(x_atom.shape, messages.shape)\n",
    "            x_atom = x_atom + messages\n",
    "            \n",
    "        return x_atom, x_bond\n",
    "\n",
    "    \n",
    "class Net(nn.Module):\n",
    "    def __init__(self, schnet):\n",
    "        super(Net, self).__init__()\n",
    "        self.schnet = schnet\n",
    "        self.agentwise = nn.Sequential(schnetpack.nn.blocks.MLP(\n",
    "            n_in=18, n_out=5, n_layers=2, activation=shifted_softplus))\n",
    "        self.softmax = torch.nn.Softmax()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        if inputs['connectivity']!=[]:\n",
    "            x_atom, x_bond = self.schnet(inputs)\n",
    "        else: x_atom = inputs['states']\n",
    "        \n",
    "        out_agentwise = self.agentwise(x_atom)\n",
    "        out_agentwise = F.softmax(out_agentwise, dim=-1)\n",
    "        #out_agentwise = self.softmax(out_agentwise)\n",
    "        return out_agentwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN NMP-PG\n",
    "\n",
    "WIDTH, HEIGHT = 5, 5\n",
    "AGENT_TYPES = ['predator', 'prey']\n",
    "N_AGENTS_EACHTYPE = [4, 1]\n",
    "FRAMES_PER_EPISODE = 30\n",
    "EPISODES_TO_TRAIN = 12000\n",
    "VERBOSE = 100\n",
    "\n",
    "model = model = Net(SchnetWithEdgeUpdate())\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "eps = np.finfo(np.float32).eps.item()\n",
    "\n",
    "# Main\n",
    "\n",
    "reward_log  = []\n",
    "\n",
    "for i_episode in range(EPISODES_TO_TRAIN):\n",
    "    # reset environment and episode reward\n",
    "    W = GridWorld(width=WIDTH, height=HEIGHT, agent_types=AGENT_TYPES, \n",
    "                  n_agents_each_type=N_AGENTS_EACHTYPE,\n",
    "                  terminal_frame=FRAMES_PER_EPISODE)\n",
    "    \n",
    "    ep_reward = 0\n",
    "    reward_buffer = [[] for _ in range(W.n_agents_each_type[0])] # each predator has 1 separate buffer\n",
    "    action_buffer = [] # throughout the whole episode\n",
    "\n",
    "    while True: # transition the environment until done\n",
    "        actions = [[0 for i in range(N)] for N in W.n_agents_each_type] # init actions in right shape\n",
    "        \n",
    "        states, connectivities, distances = [], [], []\n",
    "        \n",
    "        for agent_id, agent in enumerate(W.agents['predator']):\n",
    "            # TODO: only forward action of active predator\n",
    "            \n",
    "            view1 = W.get_view(agent.pos, agent.properties['viewrange'], 'prey').ravel()\n",
    "            view2 = W.get_view(agent.pos, agent.properties['viewrange'], 'predator').ravel()\n",
    "            state = np.hstack([view1, view2])\n",
    "            \n",
    "            # agents states\n",
    "            states.append(state)\n",
    "            \n",
    "        states = torch.tensor(states).float().to(device)\n",
    "        \n",
    "        # connectivity pairs\n",
    "        connectivity = [[i,j] for i in range(len(states)) for j in range(len(states))]\n",
    "        connectivity = np.array(connectivity)\n",
    "\n",
    "        # distance for each connectivity pair\n",
    "        distance = [1 for i in range(len(states)**2)]\n",
    "        distance = torch.tensor(distance).float().to(device)\n",
    "        \n",
    "        connectivity, distance = W.build_connectivity()\n",
    "        connectivity, distance = np.array(connectivity), np.array(distance)\n",
    "        \n",
    "        batch_data = {'states':states, 'connectivity':connectivity, 'distance':distance}\n",
    "        ###############\n",
    "\n",
    "        probs = model(batch_data) # forward pass, # probs shape: n_agents x n_action_options \n",
    "#         print(probs)\n",
    "        \n",
    "        m = Categorical(probs)\n",
    "        action_sample = m.sample() # sampled actions of all agents at this time step\n",
    "        action_buffer.append(m.log_prob(action_sample))\n",
    "\n",
    "        # embed all agents' actions into pre-initialised actions for transitioning the environment\n",
    "        actions[0] = action_sample.detach().cpu().numpy() # 0 here is index of predator\n",
    "        \n",
    "        # transition\n",
    "        W.transition(actions)\n",
    "        \n",
    "        # Get reward for each predator from the new transitioned environment\n",
    "        for agent_id in range(W.n_agents_each_type[0]):\n",
    "            reward = W.agents['predator'][agent_id].current_reward\n",
    "            reward_buffer[agent_id].append(reward)\n",
    "        \n",
    "        if W.done: break\n",
    "\n",
    "            \n",
    "    # perform backprop\n",
    "    returns = []\n",
    "    for agent_id in list(range(W.n_agents_each_type[0]))[::-1]: # Reverse order\n",
    "        R = 0\n",
    "        for r in reward_buffer[agent_id][::-1]: # Reverse order\n",
    "            R = r + 0.99 * R\n",
    "            returns.insert(0, R)\n",
    "            \n",
    "    ep_reward = np.mean(returns[::len(reward_buffer[0])]   )\n",
    "    \n",
    "    returns = torch.tensor(returns)\n",
    "    #returns = (returns - returns.mean()) / (returns.std() + eps) # this one is suspicious\n",
    "\n",
    "    action_buffer = np.transpose(action_buffer) # transpose action buffer because time step dimension is first, now needs to switch to agent dimension\n",
    "    action_buffer = [item for sublist in action_buffer for item in sublist] # ravel\n",
    "    policy_losses = []\n",
    "    for log_prob, R in zip(action_buffer, returns):\n",
    "        policy_losses.append(-log_prob * R)\n",
    "\n",
    "\n",
    "    # reset gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # sum up all the values of policy_losses and value_losses\n",
    "    loss = torch.stack(policy_losses).sum() #+ torch.stack(value_losses).sum() # ?????\n",
    "\n",
    "    # perform backprop\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # log results\n",
    "    reward_log.append(ep_reward)\n",
    "    if (i_episode+1) % VERBOSE == 0: \n",
    "        print('Episode {}\\tReward: {:.2f}'.format(i_episode+1, np.mean(reward_log[-VERBOSE:])))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
